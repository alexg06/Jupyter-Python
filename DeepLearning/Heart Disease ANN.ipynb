{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>cholesteral</th>\n",
       "      <th>thalac</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>261</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>263</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>269</td>\n",
       "      <td>121</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample  cholesteral  thalac  oldpeak   disease\n",
       "0  train          261     141        3  positive\n",
       "1  train          263     105        2  negative\n",
       "2  train          269     121        2  negative"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('heart_disease_for_curves.xls')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>cholesteral</th>\n",
       "      <th>thalac</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>261</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>263</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>269</td>\n",
       "      <td>121</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample  cholesteral  thalac  oldpeak  label\n",
       "0  train          261     141        3      1\n",
       "1  train          263     105        2      0\n",
       "2  train          269     121        2      0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(df, columns=['disease'], drop_first=True)\n",
    "df.rename(columns={'disease_positive':'label'}, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df[0:150]\n",
    "df_test = df[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>cholesteral</th>\n",
       "      <th>thalac</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>train</td>\n",
       "      <td>308</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>train</td>\n",
       "      <td>193</td>\n",
       "      <td>162</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>train</td>\n",
       "      <td>228</td>\n",
       "      <td>165</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample  cholesteral  thalac  oldpeak  label\n",
       "147  train          308     170        0      0\n",
       "148  train          193     162       19      0\n",
       "149  train          228     165       10      1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>cholesteral</th>\n",
       "      <th>thalac</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>test</td>\n",
       "      <td>231</td>\n",
       "      <td>182</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>test</td>\n",
       "      <td>262</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>test</td>\n",
       "      <td>259</td>\n",
       "      <td>130</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample  cholesteral  thalac  oldpeak  label\n",
       "150   test          231     182       38      1\n",
       "151   test          262     155        0      0\n",
       "152   test          259     130       30      1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = df_training['label']\n",
    "label_test = df_test['label']\n",
    "df_training = df_training.drop(['sample', 'label'], axis=1)\n",
    "df_test = df_test.drop(['sample', 'label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cholesteral</th>\n",
       "      <th>thalac</th>\n",
       "      <th>oldpeak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>231</td>\n",
       "      <td>182</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>262</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>259</td>\n",
       "      <td>130</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cholesteral  thalac  oldpeak\n",
       "150          231     182       38\n",
       "151          262     155        0\n",
       "152          259     130       30"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(3, activation='relu', input_dim=3))\n",
    "model.add(Dense(64, activation='relu')) #best 64 so far\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/200\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.9027 - acc: 0.5083 - val_loss: 0.8183 - val_acc: 0.6000\n",
      "Epoch 2/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.8494 - acc: 0.5250 - val_loss: 0.6862 - val_acc: 0.6000\n",
      "Epoch 3/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.8400 - acc: 0.4500 - val_loss: 0.6882 - val_acc: 0.6000\n",
      "Epoch 4/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.7543 - acc: 0.6000 - val_loss: 0.6792 - val_acc: 0.6000\n",
      "Epoch 5/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.7756 - acc: 0.5083 - val_loss: 0.8179 - val_acc: 0.6000\n",
      "Epoch 6/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.7650 - acc: 0.5000 - val_loss: 0.7170 - val_acc: 0.4667\n",
      "Epoch 7/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.7500 - acc: 0.5667 - val_loss: 0.8495 - val_acc: 0.4333\n",
      "Epoch 8/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.7408 - acc: 0.5000 - val_loss: 0.6836 - val_acc: 0.6000\n",
      "Epoch 9/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.7136 - acc: 0.5500 - val_loss: 0.7914 - val_acc: 0.5667\n",
      "Epoch 10/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.7346 - acc: 0.5333 - val_loss: 0.6817 - val_acc: 0.6000\n",
      "Epoch 11/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.7120 - acc: 0.5833 - val_loss: 0.6957 - val_acc: 0.5333\n",
      "Epoch 12/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.7038 - acc: 0.5750 - val_loss: 0.6903 - val_acc: 0.5667\n",
      "Epoch 13/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6980 - acc: 0.5667 - val_loss: 0.6796 - val_acc: 0.6333\n",
      "Epoch 14/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6984 - acc: 0.6167 - val_loss: 0.7362 - val_acc: 0.5333\n",
      "Epoch 15/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6926 - acc: 0.6083 - val_loss: 0.6805 - val_acc: 0.6333\n",
      "Epoch 16/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6963 - acc: 0.6167 - val_loss: 0.6710 - val_acc: 0.6333\n",
      "Epoch 17/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6968 - acc: 0.5833 - val_loss: 0.6887 - val_acc: 0.6000\n",
      "Epoch 18/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6871 - acc: 0.5417 - val_loss: 0.6764 - val_acc: 0.6000\n",
      "Epoch 19/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6901 - acc: 0.6250 - val_loss: 0.6757 - val_acc: 0.6000\n",
      "Epoch 20/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6863 - acc: 0.6250 - val_loss: 0.6943 - val_acc: 0.5667\n",
      "Epoch 21/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6858 - acc: 0.6167 - val_loss: 0.6806 - val_acc: 0.5667\n",
      "Epoch 22/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6793 - acc: 0.5833 - val_loss: 0.6808 - val_acc: 0.6000\n",
      "Epoch 23/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6752 - acc: 0.5750 - val_loss: 0.6622 - val_acc: 0.6000\n",
      "Epoch 24/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6769 - acc: 0.5750 - val_loss: 0.6638 - val_acc: 0.6000\n",
      "Epoch 25/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6737 - acc: 0.5833 - val_loss: 0.6618 - val_acc: 0.6333\n",
      "Epoch 26/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6715 - acc: 0.5750 - val_loss: 0.6597 - val_acc: 0.6000\n",
      "Epoch 27/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6559 - acc: 0.6500 - val_loss: 0.6932 - val_acc: 0.6000\n",
      "Epoch 28/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6604 - acc: 0.6250 - val_loss: 0.6886 - val_acc: 0.6000\n",
      "Epoch 29/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6770 - acc: 0.6583 - val_loss: 0.6674 - val_acc: 0.6000\n",
      "Epoch 30/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6697 - acc: 0.6333 - val_loss: 0.6671 - val_acc: 0.6333\n",
      "Epoch 31/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6711 - acc: 0.6167 - val_loss: 0.6533 - val_acc: 0.6333\n",
      "Epoch 32/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6720 - acc: 0.6167 - val_loss: 0.6722 - val_acc: 0.6000\n",
      "Epoch 33/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6725 - acc: 0.5917 - val_loss: 0.6588 - val_acc: 0.6333\n",
      "Epoch 34/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6780 - acc: 0.6417 - val_loss: 0.6448 - val_acc: 0.6667\n",
      "Epoch 35/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6658 - acc: 0.5917 - val_loss: 0.6503 - val_acc: 0.6333\n",
      "Epoch 36/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6575 - acc: 0.6083 - val_loss: 0.6508 - val_acc: 0.6667\n",
      "Epoch 37/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6682 - acc: 0.6250 - val_loss: 0.6432 - val_acc: 0.6333\n",
      "Epoch 38/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6532 - acc: 0.6000 - val_loss: 0.6511 - val_acc: 0.6333\n",
      "Epoch 39/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6680 - acc: 0.6583 - val_loss: 0.6397 - val_acc: 0.6667\n",
      "Epoch 40/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6615 - acc: 0.6833 - val_loss: 0.6425 - val_acc: 0.6667\n",
      "Epoch 41/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6588 - acc: 0.6500 - val_loss: 0.6344 - val_acc: 0.6333\n",
      "Epoch 42/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6540 - acc: 0.6417 - val_loss: 0.6479 - val_acc: 0.6000\n",
      "Epoch 43/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6666 - acc: 0.6500 - val_loss: 0.6309 - val_acc: 0.6667\n",
      "Epoch 44/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6649 - acc: 0.6583 - val_loss: 0.6296 - val_acc: 0.6667\n",
      "Epoch 45/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6508 - acc: 0.6417 - val_loss: 0.6763 - val_acc: 0.6667\n",
      "Epoch 46/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6592 - acc: 0.6250 - val_loss: 0.6286 - val_acc: 0.6667\n",
      "Epoch 47/200\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.6431 - acc: 0.608 - 1s 4ms/step - loss: 0.6436 - acc: 0.6083 - val_loss: 0.6306 - val_acc: 0.6667\n",
      "Epoch 48/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6326 - acc: 0.6000 - val_loss: 0.6256 - val_acc: 0.6667\n",
      "Epoch 49/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6611 - acc: 0.6750 - val_loss: 0.6255 - val_acc: 0.6667\n",
      "Epoch 50/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6125 - acc: 0.6750 - val_loss: 0.6217 - val_acc: 0.6667\n",
      "Epoch 51/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6319 - acc: 0.6500 - val_loss: 0.6373 - val_acc: 0.6667\n",
      "Epoch 52/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6530 - acc: 0.6250 - val_loss: 0.6288 - val_acc: 0.7000\n",
      "Epoch 53/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6431 - acc: 0.6333 - val_loss: 0.6181 - val_acc: 0.6667\n",
      "Epoch 54/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6235 - acc: 0.6417 - val_loss: 0.6208 - val_acc: 0.6667\n",
      "Epoch 55/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6507 - acc: 0.6417 - val_loss: 0.6202 - val_acc: 0.6667\n",
      "Epoch 56/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6344 - acc: 0.6500 - val_loss: 0.6180 - val_acc: 0.7000\n",
      "Epoch 57/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6372 - acc: 0.6250 - val_loss: 0.6121 - val_acc: 0.6667\n",
      "Epoch 58/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6476 - acc: 0.6667 - val_loss: 0.6149 - val_acc: 0.6667\n",
      "Epoch 59/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6590 - acc: 0.6500 - val_loss: 0.6171 - val_acc: 0.6667\n",
      "Epoch 60/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6228 - acc: 0.6583 - val_loss: 0.6126 - val_acc: 0.6667\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6163 - acc: 0.6500 - val_loss: 0.6709 - val_acc: 0.7000\n",
      "Epoch 62/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6486 - acc: 0.6500 - val_loss: 0.6096 - val_acc: 0.6667\n",
      "Epoch 63/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6343 - acc: 0.6583 - val_loss: 0.6264 - val_acc: 0.7000\n",
      "Epoch 64/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6319 - acc: 0.6583 - val_loss: 0.6137 - val_acc: 0.6667\n",
      "Epoch 65/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6397 - acc: 0.6750 - val_loss: 0.6085 - val_acc: 0.6667\n",
      "Epoch 66/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6332 - acc: 0.6667 - val_loss: 0.6104 - val_acc: 0.6667\n",
      "Epoch 67/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6281 - acc: 0.6333 - val_loss: 0.6072 - val_acc: 0.6667\n",
      "Epoch 68/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6068 - acc: 0.7000 - val_loss: 0.6062 - val_acc: 0.6667\n",
      "Epoch 69/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6211 - acc: 0.6583 - val_loss: 0.6389 - val_acc: 0.7000\n",
      "Epoch 70/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6313 - acc: 0.6667 - val_loss: 0.6101 - val_acc: 0.6667\n",
      "Epoch 71/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6298 - acc: 0.6417 - val_loss: 0.6124 - val_acc: 0.7333\n",
      "Epoch 72/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6421 - acc: 0.6667 - val_loss: 0.6154 - val_acc: 0.7333\n",
      "Epoch 73/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6314 - acc: 0.6750 - val_loss: 0.6302 - val_acc: 0.7333\n",
      "Epoch 74/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6253 - acc: 0.6833 - val_loss: 0.6068 - val_acc: 0.7000\n",
      "Epoch 75/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6221 - acc: 0.6750 - val_loss: 0.6370 - val_acc: 0.7333\n",
      "Epoch 76/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6290 - acc: 0.6750 - val_loss: 0.6158 - val_acc: 0.6667\n",
      "Epoch 77/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6455 - acc: 0.7250 - val_loss: 0.6137 - val_acc: 0.7333\n",
      "Epoch 78/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6132 - acc: 0.6833 - val_loss: 0.6114 - val_acc: 0.6667\n",
      "Epoch 79/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6462 - acc: 0.6500 - val_loss: 0.6076 - val_acc: 0.6667\n",
      "Epoch 80/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6339 - acc: 0.7167 - val_loss: 0.6092 - val_acc: 0.6667\n",
      "Epoch 81/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6299 - acc: 0.6500 - val_loss: 0.6050 - val_acc: 0.6667\n",
      "Epoch 82/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6291 - acc: 0.6167 - val_loss: 0.6021 - val_acc: 0.6667\n",
      "Epoch 83/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6207 - acc: 0.6583 - val_loss: 0.6199 - val_acc: 0.6667\n",
      "Epoch 84/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6298 - acc: 0.6667 - val_loss: 0.6013 - val_acc: 0.7333\n",
      "Epoch 85/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6315 - acc: 0.6917 - val_loss: 0.6022 - val_acc: 0.7000\n",
      "Epoch 86/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6097 - acc: 0.6667 - val_loss: 0.6084 - val_acc: 0.7333\n",
      "Epoch 87/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6200 - acc: 0.6917 - val_loss: 0.6571 - val_acc: 0.7000\n",
      "Epoch 88/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6346 - acc: 0.6750 - val_loss: 0.6249 - val_acc: 0.7667\n",
      "Epoch 89/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6443 - acc: 0.6333 - val_loss: 0.6097 - val_acc: 0.7667\n",
      "Epoch 90/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6213 - acc: 0.6583 - val_loss: 0.6013 - val_acc: 0.6667\n",
      "Epoch 91/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6350 - acc: 0.6583 - val_loss: 0.6020 - val_acc: 0.6667\n",
      "Epoch 92/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6315 - acc: 0.6583 - val_loss: 0.5951 - val_acc: 0.6667\n",
      "Epoch 93/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6133 - acc: 0.6583 - val_loss: 0.6017 - val_acc: 0.7667\n",
      "Epoch 94/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6375 - acc: 0.6750 - val_loss: 0.5987 - val_acc: 0.7333\n",
      "Epoch 95/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6341 - acc: 0.6750 - val_loss: 0.5987 - val_acc: 0.7667\n",
      "Epoch 96/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6341 - acc: 0.7083 - val_loss: 0.6064 - val_acc: 0.7333\n",
      "Epoch 97/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6248 - acc: 0.6667 - val_loss: 0.6020 - val_acc: 0.7000\n",
      "Epoch 98/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.5954 - acc: 0.6833 - val_loss: 0.6058 - val_acc: 0.7667\n",
      "Epoch 99/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6080 - acc: 0.6833 - val_loss: 0.6235 - val_acc: 0.6667\n",
      "Epoch 100/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6179 - acc: 0.6583 - val_loss: 0.5974 - val_acc: 0.7000\n",
      "Epoch 101/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6303 - acc: 0.6750 - val_loss: 0.5964 - val_acc: 0.6667\n",
      "Epoch 102/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6054 - acc: 0.6583 - val_loss: 0.6086 - val_acc: 0.7667\n",
      "Epoch 103/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6224 - acc: 0.6500 - val_loss: 0.5936 - val_acc: 0.6667\n",
      "Epoch 104/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6184 - acc: 0.6917 - val_loss: 0.5977 - val_acc: 0.7667\n",
      "Epoch 105/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6273 - acc: 0.6917 - val_loss: 0.6015 - val_acc: 0.7667\n",
      "Epoch 106/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6068 - acc: 0.6750 - val_loss: 0.6067 - val_acc: 0.7667\n",
      "Epoch 107/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6363 - acc: 0.7167 - val_loss: 0.6118 - val_acc: 0.7667\n",
      "Epoch 108/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6003 - acc: 0.6833 - val_loss: 0.6142 - val_acc: 0.7667\n",
      "Epoch 109/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6347 - acc: 0.6917 - val_loss: 0.6013 - val_acc: 0.7333\n",
      "Epoch 110/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6142 - acc: 0.7250 - val_loss: 0.5995 - val_acc: 0.7000\n",
      "Epoch 111/200\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.6179 - acc: 0.6583 - val_loss: 0.5986 - val_acc: 0.7000\n",
      "Epoch 112/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6229 - acc: 0.7000 - val_loss: 0.5964 - val_acc: 0.7000\n",
      "Epoch 113/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6208 - acc: 0.6833 - val_loss: 0.6029 - val_acc: 0.7667\n",
      "Epoch 114/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6131 - acc: 0.6417 - val_loss: 0.6236 - val_acc: 0.7667\n",
      "Epoch 115/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6225 - acc: 0.7083 - val_loss: 0.5919 - val_acc: 0.7000\n",
      "Epoch 116/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6089 - acc: 0.6750 - val_loss: 0.5989 - val_acc: 0.7667\n",
      "Epoch 117/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6162 - acc: 0.6750 - val_loss: 0.6112 - val_acc: 0.7667\n",
      "Epoch 118/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6121 - acc: 0.6833 - val_loss: 0.5915 - val_acc: 0.7333\n",
      "Epoch 119/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6212 - acc: 0.6917 - val_loss: 0.5934 - val_acc: 0.7000\n",
      "Epoch 120/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6076 - acc: 0.7083 - val_loss: 0.6466 - val_acc: 0.7667\n",
      "Epoch 121/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6308 - acc: 0.6833 - val_loss: 0.5992 - val_acc: 0.6667\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6345 - acc: 0.7000 - val_loss: 0.6015 - val_acc: 0.7667\n",
      "Epoch 123/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6232 - acc: 0.7000 - val_loss: 0.5982 - val_acc: 0.6667\n",
      "Epoch 124/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6161 - acc: 0.7250 - val_loss: 0.5956 - val_acc: 0.7000\n",
      "Epoch 125/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6194 - acc: 0.6833 - val_loss: 0.5950 - val_acc: 0.7000\n",
      "Epoch 126/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6243 - acc: 0.6917 - val_loss: 0.6113 - val_acc: 0.7667\n",
      "Epoch 127/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6288 - acc: 0.6750 - val_loss: 0.6177 - val_acc: 0.7667\n",
      "Epoch 128/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6125 - acc: 0.6917 - val_loss: 0.6580 - val_acc: 0.7667\n",
      "Epoch 129/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6396 - acc: 0.6667 - val_loss: 0.6016 - val_acc: 0.7667\n",
      "Epoch 130/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6188 - acc: 0.6583 - val_loss: 0.6059 - val_acc: 0.7667\n",
      "Epoch 131/200\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.6295 - acc: 0.6917 - val_loss: 0.5986 - val_acc: 0.7667\n",
      "Epoch 132/200\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.6301 - acc: 0.6833 - val_loss: 0.5911 - val_acc: 0.7333\n",
      "Epoch 133/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6255 - acc: 0.7417 - val_loss: 0.6092 - val_acc: 0.6667\n",
      "Epoch 134/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6056 - acc: 0.6667 - val_loss: 0.6427 - val_acc: 0.7667\n",
      "Epoch 135/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6383 - acc: 0.6667 - val_loss: 0.6176 - val_acc: 0.7667\n",
      "Epoch 136/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6384 - acc: 0.7083 - val_loss: 0.6054 - val_acc: 0.7667\n",
      "Epoch 137/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6153 - acc: 0.6583 - val_loss: 0.6357 - val_acc: 0.7667\n",
      "Epoch 138/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6133 - acc: 0.6833 - val_loss: 0.5935 - val_acc: 0.7000\n",
      "Epoch 139/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6307 - acc: 0.6833 - val_loss: 0.5947 - val_acc: 0.6667\n",
      "Epoch 140/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6176 - acc: 0.6833 - val_loss: 0.6384 - val_acc: 0.7667\n",
      "Epoch 141/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6339 - acc: 0.6750 - val_loss: 0.6122 - val_acc: 0.7667\n",
      "Epoch 142/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6092 - acc: 0.7000 - val_loss: 0.6019 - val_acc: 0.6667\n",
      "Epoch 143/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6165 - acc: 0.6833 - val_loss: 0.5951 - val_acc: 0.6667\n",
      "Epoch 144/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6025 - acc: 0.7083 - val_loss: 0.6301 - val_acc: 0.7667\n",
      "Epoch 145/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6209 - acc: 0.6500 - val_loss: 0.5892 - val_acc: 0.7000\n",
      "Epoch 146/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6250 - acc: 0.7167 - val_loss: 0.5874 - val_acc: 0.7000\n",
      "Epoch 147/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6210 - acc: 0.6917 - val_loss: 0.5896 - val_acc: 0.7000\n",
      "Epoch 148/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6242 - acc: 0.7000 - val_loss: 0.5975 - val_acc: 0.7667\n",
      "Epoch 149/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6213 - acc: 0.6583 - val_loss: 0.5860 - val_acc: 0.7000\n",
      "Epoch 150/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6098 - acc: 0.6333 - val_loss: 0.6004 - val_acc: 0.7667\n",
      "Epoch 151/200\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.5995 - acc: 0.6833 - val_loss: 0.6270 - val_acc: 0.7667\n",
      "Epoch 152/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6386 - acc: 0.6667 - val_loss: 0.5866 - val_acc: 0.7667\n",
      "Epoch 153/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6207 - acc: 0.6667 - val_loss: 0.5835 - val_acc: 0.7000\n",
      "Epoch 154/200\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.6116 - acc: 0.6833 - val_loss: 0.5902 - val_acc: 0.6667\n",
      "Epoch 155/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6281 - acc: 0.6833 - val_loss: 0.5864 - val_acc: 0.7333\n",
      "Epoch 156/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6221 - acc: 0.6833 - val_loss: 0.5824 - val_acc: 0.7000\n",
      "Epoch 157/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6236 - acc: 0.7083 - val_loss: 0.5879 - val_acc: 0.7667\n",
      "Epoch 158/200\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.6011 - acc: 0.6833 - val_loss: 0.6171 - val_acc: 0.6667\n",
      "Epoch 159/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6229 - acc: 0.6917 - val_loss: 0.5850 - val_acc: 0.7333\n",
      "Epoch 160/200\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.6181 - acc: 0.6667 - val_loss: 0.5957 - val_acc: 0.7667\n",
      "Epoch 161/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6119 - acc: 0.7083 - val_loss: 0.6638 - val_acc: 0.7333\n",
      "Epoch 162/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6333 - acc: 0.6750 - val_loss: 0.5854 - val_acc: 0.7000\n",
      "Epoch 163/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6222 - acc: 0.6667 - val_loss: 0.6070 - val_acc: 0.7667\n",
      "Epoch 164/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6316 - acc: 0.6833 - val_loss: 0.5951 - val_acc: 0.7667\n",
      "Epoch 165/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6259 - acc: 0.6833 - val_loss: 0.5984 - val_acc: 0.7667\n",
      "Epoch 166/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6319 - acc: 0.7083 - val_loss: 0.5954 - val_acc: 0.7667\n",
      "Epoch 167/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6244 - acc: 0.6833 - val_loss: 0.5861 - val_acc: 0.7667\n",
      "Epoch 168/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6185 - acc: 0.6667 - val_loss: 0.5845 - val_acc: 0.7333\n",
      "Epoch 169/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6145 - acc: 0.6917 - val_loss: 0.5850 - val_acc: 0.6667\n",
      "Epoch 170/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6187 - acc: 0.6833 - val_loss: 0.6058 - val_acc: 0.7667\n",
      "Epoch 171/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6152 - acc: 0.6750 - val_loss: 0.6112 - val_acc: 0.7667\n",
      "Epoch 172/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6358 - acc: 0.7250 - val_loss: 0.5866 - val_acc: 0.7333\n",
      "Epoch 173/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6174 - acc: 0.6583 - val_loss: 0.6037 - val_acc: 0.7667\n",
      "Epoch 174/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6354 - acc: 0.7000 - val_loss: 0.6066 - val_acc: 0.7667\n",
      "Epoch 175/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6154 - acc: 0.7000 - val_loss: 0.5916 - val_acc: 0.6667\n",
      "Epoch 176/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6138 - acc: 0.6917 - val_loss: 0.6093 - val_acc: 0.7667\n",
      "Epoch 177/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6144 - acc: 0.6750 - val_loss: 0.6137 - val_acc: 0.7667\n",
      "Epoch 178/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6313 - acc: 0.6667 - val_loss: 0.5875 - val_acc: 0.7667\n",
      "Epoch 179/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.5914 - acc: 0.6833 - val_loss: 0.6323 - val_acc: 0.7667\n",
      "Epoch 180/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6277 - acc: 0.7333 - val_loss: 0.5886 - val_acc: 0.6667\n",
      "Epoch 181/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6104 - acc: 0.7417 - val_loss: 0.5896 - val_acc: 0.7667\n",
      "Epoch 182/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6288 - acc: 0.6917 - val_loss: 0.5884 - val_acc: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6289 - acc: 0.6833 - val_loss: 0.5836 - val_acc: 0.7000\n",
      "Epoch 184/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6213 - acc: 0.6917 - val_loss: 0.5833 - val_acc: 0.7000\n",
      "Epoch 185/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6243 - acc: 0.7167 - val_loss: 0.5834 - val_acc: 0.7667\n",
      "Epoch 186/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6229 - acc: 0.7000 - val_loss: 0.5881 - val_acc: 0.7333\n",
      "Epoch 187/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6232 - acc: 0.7167 - val_loss: 0.5842 - val_acc: 0.7333\n",
      "Epoch 188/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6423 - acc: 0.7000 - val_loss: 0.5864 - val_acc: 0.7333\n",
      "Epoch 189/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.5985 - acc: 0.6917 - val_loss: 0.6417 - val_acc: 0.7667\n",
      "Epoch 190/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6235 - acc: 0.6917 - val_loss: 0.5953 - val_acc: 0.7667\n",
      "Epoch 191/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6218 - acc: 0.6917 - val_loss: 0.5829 - val_acc: 0.7000\n",
      "Epoch 192/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6122 - acc: 0.6833 - val_loss: 0.6005 - val_acc: 0.7667\n",
      "Epoch 193/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6250 - acc: 0.6750 - val_loss: 0.5820 - val_acc: 0.7333\n",
      "Epoch 194/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6024 - acc: 0.6750 - val_loss: 0.6295 - val_acc: 0.7667\n",
      "Epoch 195/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6219 - acc: 0.7167 - val_loss: 0.5900 - val_acc: 0.6667\n",
      "Epoch 196/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6173 - acc: 0.6417 - val_loss: 0.5825 - val_acc: 0.7333\n",
      "Epoch 197/200\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6083 - acc: 0.6917 - val_loss: 0.5961 - val_acc: 0.7667\n",
      "Epoch 198/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6129 - acc: 0.6667 - val_loss: 0.6005 - val_acc: 0.7667\n",
      "Epoch 199/200\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.6227 - acc: 0.6667 - val_loss: 0.5792 - val_acc: 0.7333\n",
      "Epoch 200/200\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.6171 - acc: 0.7167 - val_loss: 0.5796 - val_acc: 0.7333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e93ff7ba8>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(df_training,label_train, epochs=200, verbose=1, validation_split=0.2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71117234],\n",
       "       [0.20885098],\n",
       "       [0.8710605 ],\n",
       "       [0.20885098],\n",
       "       [0.20885098],\n",
       "       [0.23155707],\n",
       "       [0.20885098],\n",
       "       [0.62800974],\n",
       "       [0.67348385],\n",
       "       [0.5558104 ],\n",
       "       [0.20885098],\n",
       "       [0.8848753 ],\n",
       "       [0.97760826],\n",
       "       [0.8333071 ],\n",
       "       [0.20885098],\n",
       "       [0.20885098],\n",
       "       [0.20885098],\n",
       "       [0.93196017],\n",
       "       [0.20885098],\n",
       "       [0.506729  ],\n",
       "       [0.20885098],\n",
       "       [0.7162682 ],\n",
       "       [0.6511134 ],\n",
       "       [0.20885098],\n",
       "       [0.82972246],\n",
       "       [0.9186861 ],\n",
       "       [0.60381615],\n",
       "       [0.3245407 ],\n",
       "       [0.24328944],\n",
       "       [0.20885098],\n",
       "       [0.38691628],\n",
       "       [0.5346335 ],\n",
       "       [0.32417807],\n",
       "       [0.49853778],\n",
       "       [0.27663326],\n",
       "       [0.45746854],\n",
       "       [0.22243926],\n",
       "       [0.8076506 ],\n",
       "       [0.5346027 ],\n",
       "       [0.494408  ],\n",
       "       [0.4095768 ],\n",
       "       [0.33145115],\n",
       "       [0.23872279],\n",
       "       [0.8226919 ],\n",
       "       [0.6861783 ],\n",
       "       [0.28100613],\n",
       "       [0.56100845],\n",
       "       [0.6284926 ],\n",
       "       [0.20885098],\n",
       "       [0.20885098],\n",
       "       [0.20885098],\n",
       "       [0.20885098],\n",
       "       [0.82285446],\n",
       "       [0.8496171 ],\n",
       "       [0.20885098],\n",
       "       [0.8604422 ],\n",
       "       [0.20885098],\n",
       "       [0.20885098],\n",
       "       [0.95106786],\n",
       "       [0.41057423],\n",
       "       [0.20885098],\n",
       "       [0.59843177],\n",
       "       [0.20885098],\n",
       "       [0.20885098],\n",
       "       [0.50080913],\n",
       "       [0.9538461 ],\n",
       "       [0.20885098],\n",
       "       [0.6229435 ],\n",
       "       [0.20885098],\n",
       "       [0.32689497],\n",
       "       [0.587694  ],\n",
       "       [0.2277456 ],\n",
       "       [0.33105063],\n",
       "       [0.8611086 ],\n",
       "       [0.72482145],\n",
       "       [0.50398934],\n",
       "       [0.24172947],\n",
       "       [0.4028638 ],\n",
       "       [0.54581535],\n",
       "       [0.85362655],\n",
       "       [0.461373  ],\n",
       "       [0.20885098],\n",
       "       [0.28114864],\n",
       "       [0.20882818],\n",
       "       [0.3232764 ],\n",
       "       [0.3485984 ],\n",
       "       [0.42161068],\n",
       "       [0.83401334],\n",
       "       [0.40144637],\n",
       "       [0.33885744],\n",
       "       [0.3358223 ],\n",
       "       [0.6304487 ],\n",
       "       [0.60564846],\n",
       "       [0.22992788],\n",
       "       [0.21139392],\n",
       "       [0.20885098],\n",
       "       [0.20885098],\n",
       "       [0.5747458 ],\n",
       "       [0.20885098],\n",
       "       [0.81877816],\n",
       "       [0.94423795],\n",
       "       [0.9875924 ],\n",
       "       [0.20885098],\n",
       "       [0.8633885 ],\n",
       "       [0.20885098],\n",
       "       [0.40848434],\n",
       "       [0.3767586 ],\n",
       "       [0.7811825 ],\n",
       "       [0.20885098],\n",
       "       [0.6183991 ],\n",
       "       [0.40832627],\n",
       "       [0.39048746],\n",
       "       [0.42098078],\n",
       "       [0.2466646 ],\n",
       "       [0.3143791 ],\n",
       "       [0.20885098],\n",
       "       [0.20885098],\n",
       "       [0.29002216],\n",
       "       [0.7643254 ],\n",
       "       [0.36237717]], dtype=float32)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(df_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_predict = np.round(y_predict)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Disease  Healthy\n",
      "Disease       55       11\n",
      "Healthy       18       36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(label_test, y_predict)\n",
    "confusion = pd.DataFrame(confusion, columns=['Disease', 'Healthy'], index=['Disease', 'Healthy'])\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20e952f3e10>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y_pred_keras = model.predict(df_test).ravel()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(label_test, y_pred_keras)\n",
    "\n",
    "plt.plot(fpr_keras,tpr_keras, color='orange')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH SKLEARN MLP CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is 0.725\n",
      "         Disease  Healthy\n",
      "Disease       47       19\n",
      "Healthy       14       40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(activation='relu', alpha=0.001, solver='lbfgs', validation_fraction=0.1)\n",
    "mlp = model.fit(df_training, label_train)\n",
    "y_predict = mlp.predict(df_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "score = mlp.score(df_test, label_test)\n",
    "print('Score is {}'.format(score))\n",
    "confusion = confusion_matrix(label_test, y_predict)\n",
    "confusion = pd.DataFrame(confusion, columns=['Disease', 'Healthy'], index=['Disease', 'Healthy'])\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
