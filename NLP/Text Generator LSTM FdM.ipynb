{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"FleursDuMal.txt\"\n",
    "raw_text = open(filename, encoding=\"utf8\").read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must convert characters to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144941\n",
      "Total Vocab:  56\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144841\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0725 11:32:35.676050  3436 deprecation_wrapper.py:119] From E:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0725 11:32:35.689016  3436 deprecation_wrapper.py:119] From E:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0725 11:32:35.691012  3436 deprecation_wrapper.py:119] From E:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0725 11:32:36.003172  3436 deprecation_wrapper.py:119] From E:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0725 11:32:36.010154  3436 deprecation.py:506] From E:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0725 11:32:36.783081  3436 deprecation_wrapper.py:119] From E:\\Anaconda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0725 11:32:36.802030  3436 deprecation_wrapper.py:119] From E:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-FdM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0725 11:32:39.157090  3436 deprecation.py:323] From E:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "144841/144841 [==============================] - 11795s 81ms/step - loss: 2.6985\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.69847, saving model to weights-FdM-01-2.6985.hdf5\n",
      "Epoch 2/10000\n",
      "144841/144841 [==============================] - 12154s 84ms/step - loss: 2.2740\n",
      "\n",
      "Epoch 00002: loss improved from 2.69847 to 2.27397, saving model to weights-FdM-02-2.2740.hdf5\n",
      "Epoch 3/10000\n",
      "144841/144841 [==============================] - 12013s 83ms/step - loss: 2.0460\n",
      "\n",
      "Epoch 00003: loss improved from 2.27397 to 2.04600, saving model to weights-FdM-03-2.0460.hdf5\n",
      "Epoch 4/10000\n",
      "144841/144841 [==============================] - 12006s 83ms/step - loss: 1.8801\n",
      "\n",
      "Epoch 00004: loss improved from 2.04600 to 1.88005, saving model to weights-FdM-04-1.8801.hdf5\n",
      "Epoch 5/10000\n",
      "144841/144841 [==============================] - 11971s 83ms/step - loss: 1.7437\n",
      "\n",
      "Epoch 00005: loss improved from 1.88005 to 1.74369, saving model to weights-FdM-05-1.7437.hdf5\n",
      "Epoch 6/10000\n",
      "144841/144841 [==============================] - 11993s 83ms/step - loss: 1.6255\n",
      "\n",
      "Epoch 00006: loss improved from 1.74369 to 1.62555, saving model to weights-FdM-06-1.6255.hdf5\n",
      "Epoch 7/10000\n",
      "144841/144841 [==============================] - 12220s 84ms/step - loss: 1.5245\n",
      "\n",
      "Epoch 00007: loss improved from 1.62555 to 1.52448, saving model to weights-FdM-07-1.5245.hdf5\n",
      "Epoch 8/10000\n",
      "144841/144841 [==============================] - 12132s 84ms/step - loss: 1.4259\n",
      "\n",
      "Epoch 00008: loss improved from 1.52448 to 1.42589, saving model to weights-FdM-08-1.4259.hdf5\n",
      "Epoch 9/10000\n",
      "144841/144841 [==============================] - 12413s 86ms/step - loss: 1.3317\n",
      "\n",
      "Epoch 00009: loss improved from 1.42589 to 1.33167, saving model to weights-FdM-09-1.3317.hdf5\n",
      "Epoch 10/10000\n",
      "144841/144841 [==============================] - 12000s 83ms/step - loss: 1.2477\n",
      "\n",
      "Epoch 00010: loss improved from 1.33167 to 1.24772, saving model to weights-FdM-10-1.2477.hdf5\n",
      "Epoch 11/10000\n",
      "144841/144841 [==============================] - 11816s 82ms/step - loss: 1.1606\n",
      "\n",
      "Epoch 00011: loss improved from 1.24772 to 1.16064, saving model to weights-FdM-11-1.1606.hdf5\n",
      "Epoch 12/10000\n",
      "144841/144841 [==============================] - 11676s 81ms/step - loss: 1.0868\n",
      "\n",
      "Epoch 00012: loss improved from 1.16064 to 1.08685, saving model to weights-FdM-12-1.0868.hdf5\n",
      "Epoch 13/10000\n",
      "144841/144841 [==============================] - 11502s 79ms/step - loss: 1.0104\n",
      "\n",
      "Epoch 00013: loss improved from 1.08685 to 1.01043, saving model to weights-FdM-13-1.0104.hdf5\n",
      "Epoch 14/10000\n",
      "144841/144841 [==============================] - 11384s 79ms/step - loss: 0.9461\n",
      "\n",
      "Epoch 00014: loss improved from 1.01043 to 0.94607, saving model to weights-FdM-14-0.9461.hdf5\n",
      "Epoch 15/10000\n",
      "144841/144841 [==============================] - 11341s 78ms/step - loss: 0.8922\n",
      "\n",
      "Epoch 00015: loss improved from 0.94607 to 0.89216, saving model to weights-FdM-15-0.8922.hdf5\n",
      "Epoch 16/10000\n",
      "144841/144841 [==============================] - 11300s 78ms/step - loss: 0.8313\n",
      "\n",
      "Epoch 00016: loss improved from 0.89216 to 0.83133, saving model to weights-FdM-16-0.8313.hdf5\n",
      "Epoch 17/10000\n",
      "144841/144841 [==============================] - 11193s 77ms/step - loss: 0.7801\n",
      "\n",
      "Epoch 00017: loss improved from 0.83133 to 0.78006, saving model to weights-FdM-17-0.7801.hdf5\n",
      "Epoch 18/10000\n",
      "144841/144841 [==============================] - 11154s 77ms/step - loss: 0.7381\n",
      "\n",
      "Epoch 00018: loss improved from 0.78006 to 0.73811, saving model to weights-FdM-18-0.7381.hdf5\n",
      "Epoch 19/10000\n",
      "144841/144841 [==============================] - 11076s 76ms/step - loss: 0.7055\n",
      "\n",
      "Epoch 00019: loss improved from 0.73811 to 0.70555, saving model to weights-FdM-19-0.7055.hdf5\n",
      "Epoch 20/10000\n",
      "144841/144841 [==============================] - 11056s 76ms/step - loss: 0.6600\n",
      "\n",
      "Epoch 00020: loss improved from 0.70555 to 0.65997, saving model to weights-FdM-20-0.6600.hdf5\n",
      "Epoch 21/10000\n",
      "144841/144841 [==============================] - 11004s 76ms/step - loss: 0.6315\n",
      "\n",
      "Epoch 00021: loss improved from 0.65997 to 0.63146, saving model to weights-FdM-21-0.6315.hdf5\n",
      "Epoch 22/10000\n",
      "144841/144841 [==============================] - 10989s 76ms/step - loss: 0.6120\n",
      "\n",
      "Epoch 00022: loss improved from 0.63146 to 0.61200, saving model to weights-FdM-22-0.6120.hdf5\n",
      "Epoch 23/10000\n",
      "144841/144841 [==============================] - 10889s 75ms/step - loss: 0.5847\n",
      "\n",
      "Epoch 00023: loss improved from 0.61200 to 0.58471, saving model to weights-FdM-23-0.5847.hdf5\n",
      "Epoch 24/10000\n",
      "144841/144841 [==============================] - 10847s 75ms/step - loss: 0.5674\n",
      "\n",
      "Epoch 00024: loss improved from 0.58471 to 0.56741, saving model to weights-FdM-24-0.5674.hdf5\n",
      "Epoch 25/10000\n",
      "144841/144841 [==============================] - 10765s 74ms/step - loss: 0.5412\n",
      "\n",
      "Epoch 00025: loss improved from 0.56741 to 0.54119, saving model to weights-FdM-25-0.5412.hdf5\n",
      "Epoch 26/10000\n",
      "144841/144841 [==============================] - 10792s 75ms/step - loss: 0.5198\n",
      "\n",
      "Epoch 00026: loss improved from 0.54119 to 0.51976, saving model to weights-FdM-26-0.5198.hdf5\n",
      "Epoch 27/10000\n",
      "144841/144841 [==============================] - 10651s 74ms/step - loss: 0.5093\n",
      "\n",
      "Epoch 00027: loss improved from 0.51976 to 0.50930, saving model to weights-FdM-27-0.5093.hdf5\n",
      "Epoch 28/10000\n",
      "144841/144841 [==============================] - 10602s 73ms/step - loss: 0.4933\n",
      "\n",
      "Epoch 00028: loss improved from 0.50930 to 0.49331, saving model to weights-FdM-28-0.4933.hdf5\n",
      "Epoch 29/10000\n",
      "144841/144841 [==============================] - 10449s 72ms/step - loss: 0.4822\n",
      "\n",
      "Epoch 00029: loss improved from 0.49331 to 0.48221, saving model to weights-FdM-29-0.4822.hdf5\n",
      "Epoch 30/10000\n",
      "144841/144841 [==============================] - 10660s 74ms/step - loss: 0.4649\n",
      "\n",
      "Epoch 00030: loss improved from 0.48221 to 0.46494, saving model to weights-FdM-30-0.4649.hdf5\n",
      "Epoch 31/10000\n",
      " 30016/144841 [=====>........................] - ETA: 2:19:06 - loss: 0.3954"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c3cb68c99cb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model.load_weights('weights-conan-338-0.3525.hdf5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.load_weights('weights-conan-338-0.3525.hdf5')\n",
    "model.fit(X, y, epochs=10000, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-Fdm-165-0.6485.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "ed ligenn and mands oa ladrest were ruehtere whsh derteon samagl reacrtert th t toraha mened anhin the pratss in stiels  and the\n",
      "lltedom of the plder saae hi the hyrkanians, nuweaned byoger oe ã†rir and vanir swarmed into the lands, and the pictish eslans and thnugm poprter, the brrnindaney  femecin whrh the besserians ale sordicas whi hape intl teemr line tmet ay the hererialis of sueges cadr on the rears of tae sraless aadrea toygia  nnten thay horg the lars thav ate tham th ted wivh thets la wac sestey oftoon weto crue ly andis, the\n",
      "newer kingdom is called ntygiaa ir anuilonia and wast elcatey, the\n",
      "western world. rhiue werl\n",
      "nohrinle iyteriory ges ont came tn eupse th  soun of the basbarians of\n",
      "the aasbarians hf\n",
      "tch saaee- tiey iad alrn eo name tal and plunder. the racr of the hypkanian kiogs.ms. \n",
      "shil and tavages whah asunlunia'aad wete botthered enmmle iioo mtrerilg banore them, theu siey nor ralas abee to mheeu, which had eer tortin sort wort clpeled trth the nome cabkai aroise oh\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "#print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
